{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53a6fb4a-75af-4d06-bc36-d8a1af30d314",
   "metadata": {},
   "source": [
    "## Day 4 Hypothesis testing, Interval estimation\n",
    "\n",
    "### 0. Contents\n",
    "Hypothesis testing: Likelihood-Ratio test\n",
    "\n",
    "Inteerval estimation: likelihood-based inference on one paramter. Joint confidence region for two paramters. Profile likelihood. Wald C.I. with normal approximation.  \n",
    "### 1. Statistical inference\n",
    "Statistical inference often refers to the procedures of point estimation, interval estimation, and hypothesis testing. For point estimation, it simply means to finding the best guess for a parameter. This is achieved by maximising the log-likelihood function as described in the previous notebook. The point estimate is usually accompanied by a confidence interval (C.I.) to reflect how certain we are surrounding the \"best guess\". Hypothesis testing is heavily involved in model selection and simplification, or testing for the difference between treatment groups in a clinical trial. Luckily, likelihood has all these covered. \n",
    "\n",
    "### 2. Likelihood-Ratio test\n",
    "Likelihood-Ratio test (LRT) is a likelihood-based hypothesis testing procedure. It applies to two *nested* models aiming to decide which one we should choose from. Let M1 and M2 be two models, and that M1 is nested in M2 (i.e. M1 is a special case of M2). We also assume $d1$ and $d2$ are the number of free parameters for M1 and M2 respectively, with $d2>d1$. The LRT statistic $D$ is twice the difference of their maximised log-likelihoods, and $D$ approximately follows a $\\chi^2$ distribution with $(d2-d1)$ degrees of freedom. \n",
    "\n",
    "The procedure of performing a LRT is as follows: \n",
    "1) Fit M1 to the data, record the maximised log-likelihood value, let us call it $\\ln(L1)$. The parameter estimates are unimportant here. \n",
    "2) Fit M2 to the same data, separatly, record the maximised log-likelihood value $\\ln(L2)$. Note that $\\ln(L2)\\geqslant\\ln(L1)$. \n",
    "3) Compute the LRT statitsic $D=2[\\ln(L2)-\\ln(L1)]$. \n",
    "4) Look up $\\chi^2_{d2-d1}$ table for a critical value. Accept M1 as the simplidfied model if $D$ is smaller than the critical value.\n",
    "\n",
    "The intuition of $D$ is to measure the difference in explanatory power between the two models. M2, being the full model, must fit the data better with a larger maximised log-likelihood. M1 has fewer parameters hence poorer fit. If the parameters dropped by M1 are unimportant, then there should be little difference between the two models, hence a small value of $D$. With small $D$ we tend to accept M1 as the simplified model. \n",
    "\n",
    "In today's practical we will apply LRT to test for the signficance of the intercept term in a linear regression. In this case, the full model M2 is the three-parameter model we fitted yesterday, whereas M1 is the simplified model with $a=0$. We will also revisit the coin tossing example to test for (or against) the fair-coin hypothesis. \n",
    "\n",
    "### 3. Interval estimation\n",
    "#### 3.1 Likelihood-based confidence interval\n",
    "Following on the discussion on LRT, it will accept the simpler model if $D$ is smaller than half of the critical value of $\\chi^2_1$, assuming M1 and M2 differ by one parameter. At $\\alpha=0.05$, the critical value is 3.84, hence the decision boundary is $D<3.84/2=1.92$, or 2 units if we are a bit conservative. \n",
    "\n",
    "With this idea, we can perform a series of LRTs against many M1 with different $\\tilde{\\theta}$ values. As we move $\\tilde{\\theta}$ further away from $\\hat{\\theta}$, the $D$ statistic will get larger and eventually hit the decision boundary to reject M1. As a result, the 95% C.I. for $\\theta$ in the univariate case is a collection of $\\tilde{\\theta}$ values whose M1 remain accepted by LRT. Or in other words, a collection of $\\tilde{\\theta}$ whose log-likelihood is within 1.92 or 2 units from the peak of the log-likelihood curve $l(\\hat{\\theta})$. \n",
    "\n",
    "Example: In a coin tossing experiment we obtained 7 heads and 3 tails. On day 2 we visualised the log-likelihood function. We also learnt that the maximised log-likelihood is $l(\\hat{p})=l(0.7)$. If we draw a horizontal line 2 units below the peak: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "649f8f9c-8e39-4ec4-80ae-10a625ad2e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAOVBMVEX9/v0AAAAAAP9MTUxn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\nEgQICQKEBAFCggAhQcD/A3xqKe5dnuszAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# THE COIN TOSSING LOG-LIKELIHOOD FUNCTION\n",
    "log.binomial.likelihood<-function(p) {lchoose(10,7)+7*log(p)+3*log(1-p)}\n",
    "p<-seq(0, 1, 0.01)\n",
    "log.likelihood.values<-log.binomial.likelihood(p)\n",
    "plot(p, log.likelihood.values, ylab='log-likelihood', type='l', lwd=2)\n",
    "# DRAW A LINE 2 UNITS BELOW THE PEAK\n",
    "abline(h=log.binomial.likelihood(0.7)-2, col='red', lty=2, lwd=2)\n",
    "# THE 95% C.I.\n",
    "abline(v=c(0.39, 0.92), col='blue', lty=3, lwd=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba522ab-3ab9-4c9e-bbde-6189a43225da",
   "metadata": {},
   "source": [
    "The intersections between the log-likelihood and the minus-two line are the lower and upper 95% C.I. for $p$, in this case, [0.39, 0.92]. If you want to be ultra precise, you can use <code>uniroot()</code> to find the intersection points. \n",
    "\n",
    "#### 3.2 Joint confidence region for a two-parameter model\n",
    "Obtaining individual C.I.s does not mean we also have found the *joint* confidence region for two or more parameters, as the overall $\\alpha$ level cannot be maintained when making multiple comparisons. Corrections (e.g. Bonferroni) can be applied to individual tests to preserve the overall $\\alpha$, but the details are not discussed here. Another concern is the correlation among ML estimators. \n",
    "\n",
    "The log-likelihood surface for a two-parameter model can be visualised by a 3D plot (via <code>persp()</code>) or a <code>contour()</code> plot. The 95% C.I. for *one* parameter alone can be found using the same -1.92 rule as desbribed above. For the 95% joint confidence region for *both* parameters, we look for a collection of parameter pairs whose log-likelihood is within $0.5*\\chi^2_{2, 0.95}=2.99$ units from the maximum. See #4 of <code>recapture.csv</code> notebook for example. \n",
    "\n",
    "#### 3.3 Profile likelihood \n",
    "Profiling is a generalisation of #3.2. Let us partition the vector of parameters into two subsets: $\\underline{\\theta}=(\\underline{\\theta_1},\\underline{\\theta_2})$. The motivation is to find the C.I. (or region) for $\\underline{\\theta_1}$. The procedure involves *partial* maximisation of the original log-likelihood along $\\underline{\\theta_1}$. If we fix $\\underline{\\theta_1}=\\underline{\\tilde{\\theta_1}}$, then we can vary $\\underline{\\theta_2}$ such that the log-likelihood is partially maximised. And this value is the profile log-likelihood value at $\\underline{\\tilde{\\theta_1}}$ (we can call it $l^*(\\underline{\\tilde{\\theta_1}})$). And if repeat the same for many other $\\underline{\\tilde{\\theta_1}}$, then we obtain a profile log-likelihood function for the parameter subset $\\underline{\\theta_1}$. \n",
    "\n",
    "Mathematically,$$l^*(\\underline{\\theta_1})=\\max_{\\underline{\\theta_2}} l(\\underline{\\theta_1}, \\underline{\\theta_1}; \\underline{x})$$\n",
    "\n",
    "See today's practical on <code>flowering.txt</code>. The general rule for the 95% joint C.I. (region) for $k$ parameters is the collection of parameter values for which the log-likelihood decreases by no more than half of $\\chi^2_{0.95~df=k}$ from its maximum. \n",
    "\n",
    "#### 3.4 Confidence interval via normal approximation (Wald C.I.)\n",
    "##### 3.4.1 Univartiate Wald C.I.\n",
    "One key property of ML estimators is approximate normality under reasonably large sample size. It means that the 95% C.I. for the true parameter $\\theta$ is approximately $\\hat{\\theta}\\pm1.96\\sqrt{Var(\\hat{\\theta})}$. The magic number 1.96 comes from the 2.5- and 97.5-percentile of the standard normal distribution. Wald C.I. (or tests) are extensively used <code>lm()</code> and <code>glm()</code> to indicate the significance of explanatory variables. Have you wondered how the standard errors from a <code>summary()</code> table are computed? In fact, they can be approximated from the curvature of the log-likelihood:\n",
    "$$Var(\\hat{\\theta})\\approx-\\frac{1}{l''(\\hat{\\theta})}$$\n",
    "where $l''(\\hat{\\theta})$ is the second derivative of the log-likelihood function evaluated at $\\hat{\\theta}$. Note that the second derivative is always negative given it is a maximum therefore a -1 is required to ensure $Var(\\hat{\\theta})$ is positive. If the log-likelihood is steeper (i.e. descends faster) around its peak then the C.I. should be narrower, thus smaller $Var(\\hat{\\theta})$. Therefore \"steepness\" and variance are inversely proportional as reflected in the formula. \n",
    "\n",
    "##### 3.4.2 Multivariate case\n",
    "The principle still holds for multivariate (multiple parameter) case as the ML estimators jointly follow a multivariate normal. In multivariate normal, the \"variance\" is no longer a number but a variance-covariance matrix $\\boldsymbol{V}(\\underline{\\hat{\\theta}})$, with rows and columns equal the number of parameters. Similar to the univariate case, it is related to the second derivative of the log-likelihood surface. Empirically, \n",
    "$$\\boldsymbol{V}(\\underline{\\hat{\\theta}})\\approx-\\boldsymbol{H}(\\underline{\\hat{\\theta}})^{-1}$$\n",
    "$\\boldsymbol{H}(\\underline{\\hat{\\theta}})$ is the Hessian matrix, the second derivative of the $l(\\underline{\\theta})$ evaluated at $\\hat{\\theta}$. It sounds familiar as we can request <code>hessian=T</code> from <code>optim()</code>. $-\\boldsymbol{H}(\\underline{\\hat{\\theta}})$ is also called the observed Fisher information matrix, which tells the amount of information contained towards the parameters. \n",
    "\n",
    "The final step is to calculate the negative inverse of the Hessian matrix, which can be completed via the <code>solve()</code> function. With $\\boldsymbol{V}(\\underline{\\hat{\\theta}})$ we can infer the Wald C.I. for a single parameter under the usual way, or perform multivariate testing for a collection of parameters. \n",
    "\n",
    "“In Author’s experience, the Wald (i.e. normality) and likelihood method can give quite different results when used to test joint hypotheses… The likelihood method can requie more effort to compute, but is generally preferred.” (Milalr 2011)\n",
    "\n",
    "##### 3.5 Interpretations of C.I.\n",
    "Classical statistics considers a parameter $\\theta$ as a fixed but unknown number. If the experiment is repeated for infinitely many times and that infinitely many $\\hat{\\theta}$ and 95% C.I. are obtained, then 95% of the C.I.s will cover the true parameter value. \n",
    "\n",
    "This concept is completely revolutionised by the Bayesian branch, that it (weirdly) believes $\\theta$ can also be a r.v. thus has its own distributions (e.g. prior and posterior distributions). Therefore, making probabilistic statements towards $\\theta$ is permitted (i.e. there is 95% chance that $\\theta$ is between such as such value). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ce27f-0abd-46b6-a36d-ab849869a864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R-4.3.1",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
