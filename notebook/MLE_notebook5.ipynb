{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "745aad23-dfc4-472f-aa04-c9f0e531812d",
   "metadata": {},
   "source": [
    "# Selected topics\n",
    "### 0. Contents\n",
    "Selected topics: transformation of random variables, nuisance variables, sufficient statistic, algorithms to generating random numbers (inverse transform and rejection method), Monte-Carlo integration, Markov chain, Markov chain Monte-Carlo\n",
    "\n",
    "Moment estimators. Random effect model (integrated likelihood). \n",
    "\n",
    "### 1. Selected topics in probability and statistics\n",
    "Before I proceed I would like to thank Fred Chen and Oksana Vertsimakha for being the GTA this year. \n",
    "\n",
    "Previous CMEE students asked about these topics therefore I decided to share them with you as well. These topics may not be related so they can be studied in any order. \n",
    "\n",
    "#### 1.1 Transformation of random variables\n",
    "Let $X$ be a r.v. with pdf $f_X(x)$. $Y=g(X)$ is also a r.v.. If $g$ is a one-to-one onto function then the pdf of $Y$ is \n",
    "$$f_Y(y)=f_X(g^{-1}(y))|\\frac{d}{dy}g^{-1}(y)|$$\n",
    "\n",
    "There is a similar formula for multivariate transformation which involes partial derivatives (Jacobian matrix). \n",
    "\n",
    "#### 1.2 Nuisance variables\n",
    "Given two r.v. $Y$ and $U$. Assume we know $Y|U$ has pdf $f_{Y|U}(y|u)$. $U$ is another r.v. with pdf $f_U(u|\\theta)$. $\\theta$ is a paramter (number). The goal is to find the distributin of $Y$ without referencing to the intermediate r.v. $U$. Here $U$ is often regarded as a nuisance or latend variable. \n",
    "$$f_Y(y|\\theta)=\\int f_{Y|U}(y|u)f_U(u|\\theta)du$$\n",
    "Note that here we integrate over the support of $U$. This result follows the \"joint = marginal x conditional\" relationship we discussed earlier.\n",
    "\n",
    "#### 1.3 Sufficient statistic\n",
    "A sufficient statistic (a statistic is a number, statistics is a subject) summarises all information provided by the observations towards a parameter. It is the core principle of data reduction. For example in estimating $\\sigma^2$ from $n$ i.i.d. $N(\\mu, \\sigma^2)$ samples we only need to know $\\sum{x_i}/n$ and $\\sum{x_i^2}/n$. There is no need to go through individual values. Therefore the two are sufficient statistics of $\\sigma^2$. \n",
    "\n",
    "#### 1.4 Generating random numbers via inverse transform\n",
    "Suppose our goal is to generate some random numbers that follow a specific distribution, say, $X\\sim f_X(x)$, but we only have the uniform $U(0, 1)$ generator. The algorithm is as follows: \n",
    "1) find $F_X$, the cdf of $X$\n",
    "2) sample $u\\sim U(0, 1)$, then $F_X^{-1}(u)$ follows the intended $f_X$\n",
    "\n",
    "#### 1.5 Generating random numbers via rejection algorithm\n",
    "Suppose we want to generate $X\\sim f_X(x)$ from a proposal distribution $Y\\sim f_Y(y)$. This requires the support of $Y$ to include the support of $X$. The algorithm is as follows:\n",
    "1) find a constant number $M$, such that $f_X(x)<=M*f_Y(x)$ for all $x$ within its support. In other words, plot the pdfs of $X$ and $Y$ on the same graph, rescale $f_Y$ with a multiplyer $M$ such that $M*f_Y$ envelopes $f_X$. \n",
    "2) sample $y\\sim f_Y$\n",
    "3) sample $u\\sim U(0, 1)$\n",
    "4) if $u<\\frac{f_X(y)}{M*f_Y(y)}$ then accept this $y$, discard otherwise\n",
    "\n",
    "Those accepted will follow $f_X$. \n",
    "\n",
    "#### 1.6 Monte-Carlo integration\n",
    "Suppose our goal is to evaluate the following integral: \n",
    "$$I=\\int_0^1\\sqrt{1-x^2}dx$$\n",
    "You can calculate the integral by hand (!), or by quadrature rule (counting rectangles). For instance, via <code>integrate()</code> from R:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac5aac85-987a-4c14-8780-e4052c2d9bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7853983 with absolute error < 0.00011"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "integrate(function(x){sqrt(1-x^2)}, lower=0, upper=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0528d233-e40d-47cf-99c2-e9c6c1f714aa",
   "metadata": {},
   "source": [
    "These methods are mostly deterministic. Since we know how to draw random numbers from R, here I propose an alternative approach for the same problem: \n",
    "1) sample $n$ numbers from $U(0, 1)$\n",
    "2) compute $I_n=\\frac{1}{n}\\sqrt{1-x_i^2}$\n",
    "3) If $n$ is sufficiently large then $I_n$ is a good approximation to the original integral $I$\n",
    "\n",
    "In general, if we want to find $I=\\int_a^bg(x)dx$, it is equivalent to finding\n",
    "$$I=\\int_a^bg(x)dx=\\int_a^b\\frac{g(x)}{f(x)}f(x)dx$$\n",
    "$$=E_X[\\frac{g(X)}{f(X)}]$$\n",
    "the expectation of the transformed r.v. $g(X)/f(X)$, where $X$ has pdf $f$. Note that the support of the proposed r.v. $X$ must include $(a,b)$. Remember expectation is the hypothetical average of infinitely many trials, which is \"achievable\" by computer. This also justifies the Monte-Carlo simulations we did in the Evolutionary Modelling week. \n",
    "\n",
    "I hope you appreciate the simplicity of MC methods. It is also a beauty to use some random numbers to approximate a deterministic quantity. MC integration works for $\\pm\\infty$ bounds as long as a suitable r.v. is chosen (e.g. normal). \n",
    "Of course, the stochastic nature of Monte Carlo integration means there will be errors, and convergence can be slow in multiple integrals (with multivariate r.v.). \n",
    "\n",
    "#### 1.7 Markov Chain\n",
    "Markov Chain is a chain of time-dependent r.v., $X(t), X(t+1), X(t+2), ...$. There are several states (possible outcomes, discrete or continuous) that $X(j)$ can take on. For example, the allele count of a locus over time, whose states can be ${0, 1, 2, ..., 2N}$ for a population of $N$ diploids. Another example is the size of a population in a birth and death process. When there is a birth event, the population size moves up to the next value (state), and down by one when a death event occurs. \n",
    "\n",
    "The chain transits from one state to another by chance over time, and the transition probabilites depend only on the current state. Often the probabilities are put in a Markov matrix $M$ with number of rows and columns equal the number of states. A time-homogeneous Markov chain is a special case whose transition probabilties remain the same over time. \n",
    "\n",
    "The Wright-Fisher model can be analysed as a time-homogeneous Markov Chain. For example, with $N=2$, there are 5 states for the allele counts. Its Markov matrix is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc19e4c2-2a30-410e-9e6b-35c1974309be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 5 × 5 of type dbl</caption>\n",
       "<tbody>\n",
       "\t<tr><td>1.00000000</td><td>0.000000</td><td>0.0000000</td><td>0.000000</td><td>0.00000000</td></tr>\n",
       "\t<tr><td>0.31640625</td><td>0.421875</td><td>0.2109375</td><td>0.046875</td><td>0.00390625</td></tr>\n",
       "\t<tr><td>0.06250000</td><td>0.250000</td><td>0.3750000</td><td>0.250000</td><td>0.06250000</td></tr>\n",
       "\t<tr><td>0.00390625</td><td>0.046875</td><td>0.2109375</td><td>0.421875</td><td>0.31640625</td></tr>\n",
       "\t<tr><td>0.00000000</td><td>0.000000</td><td>0.0000000</td><td>0.000000</td><td>1.00000000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 5 × 5 of type dbl\n",
       "\\begin{tabular}{lllll}\n",
       "\t 1.00000000 & 0.000000 & 0.0000000 & 0.000000 & 0.00000000\\\\\n",
       "\t 0.31640625 & 0.421875 & 0.2109375 & 0.046875 & 0.00390625\\\\\n",
       "\t 0.06250000 & 0.250000 & 0.3750000 & 0.250000 & 0.06250000\\\\\n",
       "\t 0.00390625 & 0.046875 & 0.2109375 & 0.421875 & 0.31640625\\\\\n",
       "\t 0.00000000 & 0.000000 & 0.0000000 & 0.000000 & 1.00000000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 5 × 5 of type dbl\n",
       "\n",
       "| 1.00000000 | 0.000000 | 0.0000000 | 0.000000 | 0.00000000 |\n",
       "| 0.31640625 | 0.421875 | 0.2109375 | 0.046875 | 0.00390625 |\n",
       "| 0.06250000 | 0.250000 | 0.3750000 | 0.250000 | 0.06250000 |\n",
       "| 0.00390625 | 0.046875 | 0.2109375 | 0.421875 | 0.31640625 |\n",
       "| 0.00000000 | 0.000000 | 0.0000000 | 0.000000 | 1.00000000 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]       [,2]     [,3]      [,4]     [,5]      \n",
       "[1,] 1.00000000 0.000000 0.0000000 0.000000 0.00000000\n",
       "[2,] 0.31640625 0.421875 0.2109375 0.046875 0.00390625\n",
       "[3,] 0.06250000 0.250000 0.3750000 0.250000 0.06250000\n",
       "[4,] 0.00390625 0.046875 0.2109375 0.421875 0.31640625\n",
       "[5,] 0.00000000 0.000000 0.0000000 0.000000 1.00000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WF<-function(N)\n",
    "{\n",
    "    k<-seq(0, 2*N, 1)\n",
    "    f<-function(x, y) {dbinom(y, size=2*N, prob=x/(2*N))}\n",
    "    return(outer(k, k, f))\n",
    "}\n",
    "WF(2) # WRIGHT-FISHER MAXTIX WITH N=2. 5 STATES: 0, 1, 2, 3, 4 COPIES OF ALLELES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dc67e4-e72f-403c-a564-c29e9a003deb",
   "metadata": {},
   "source": [
    "The one-step transition probability can be found from $\\boldsymbol{M}$ directly. For instance, the probability of having 4 copies of an allele in the next generation given there is only one is 0.00390625. In the WF matrix, the first and the last rows are the absorbing states where you are stuck once you have entered. These two refer to the cases of extinction (0 copy) and fixation (4 copies) of an allele. \n",
    "\n",
    "Many analyses can be performed with $\\boldsymbol{M}$. First, $\\boldsymbol{M}$ has to be non-negative, and rows must sum to one. Some $\\boldsymbol{M}$ may have a limiting distribution, that $\\lim_{T\\rightarrow\\infty}\\boldsymbol{M}^T$ exists. Some may even have a stationary distribution $\\pi$, where $\\pi \\boldsymbol{M}=\\pi$. \n",
    "\n",
    "#### 1.8 Markov Chain Monte-Carlo integration\n",
    "In #1.6 we used a sequence of i.i.d. random numbers ${x_1, x_2, ..., x_n}$ to approximate an integral. That is, \n",
    "$$\\int_a^b g(x)dx\\approx\\frac{1}{n}\\sum\\frac{g(x_i)}{f(x_i)}$$\n",
    "In fact, the approximation still holds for *dependent* numbers. In some (usually multidimensional) cases, it is easier to construct a Markov chain, whose stationary distribution is $f$, from which Monte-Carlo samples are drawn. These MCMC (Markov chain + Monte-Carlo) algorithms include Gibbs sampling and Metropolis-Hasting (MH), which are to be introduced in the Bayesian module. \n",
    "\n",
    "### 2. Moment estimators\n",
    "Although the MLE is the best method to find estimators it often fails to provide closed-form solutions. One example is gamma samples, where the exact MLE cannot be derived by hand even the log-likelihood is known. Let $X_1, X_2, ..., X_n\\sim Gamma(\\alpha, \\beta)$, i.i.d.. The log-likelihood function for the two paramters is:\n",
    "$$l(\\alpha, \\beta)=n\\alpha\\ln(\\beta)-n\\ln(\\Gamma(\\alpha))+(\\alpha-1)\\sum\\ln(x_i)-\\beta\\sum{x_i}$$\n",
    "where $\\Gamma(\\alpha)$ is the gamma function. The derivatives of the gamma function often involves the digamma function which cannot be evaluated explicitly. As a result, the MLE $(\\hat{\\alpha}, \\hat{\\beta})$ can only be evaluated numerically, say, via <code>optim()</code>. \n",
    "\n",
    "There exist other estimators such as the method of moments. It matches population moments with sample moments. In the example of gamma, there are two parameters, hence using the first two moments should suffice. The first two population moments of a gamma r.v. are:\n",
    "$$E[X_i]=\\frac{\\alpha}{\\beta}$$\n",
    "$$E[X_i^2]=\\frac{\\alpha(\\alpha+1)}{\\beta^2}$$\n",
    "These moments can be derived from its pdf or mgf. Now we turn to the sample moments, which come from our samples, of course: \n",
    "$$m_1=\\sum_{i=1}^n{x_i}$$\n",
    "$$m_2=\\sum_{i=1}^n{x_i^2}$$\n",
    "The method of moments matches the population moments with the sample moments:\n",
    "$$\\{\\begin{array}{c} \\frac{\\alpha}{\\beta}=m_1 \\\\ \\frac{\\alpha(\\alpha+1)}{\\beta^2}=m_2\\end{array}$$\n",
    "The remaining task is to find the pair of $(\\tilde{\\alpha}, \\tilde{\\beta})$ that satisfies this system of equations. And they are our gamma moment estimators. Note that they do not necessarily enjoy the five properties of ML estimators we discussed in Day 3. \n",
    "\n",
    "### 3. Random effect model\n",
    "In linear models (or GLMs) the explanatory variables are *fixed* effects as they are factors that we have control on or would like to investigate. In contrast, *random* effects have factor levels that are drawn from a large pool in which the individuals vary in many ways, but we do not know exactly how or why they differ. \n",
    "\n",
    "Last week I was analysing the data from an egg laying experiment which involves the egg counts and their hatching rates from five mosquito crosses (four transgene and one wildtype). The cross is certainly a fixed effect as we would hope that the transgene crosses are just as fit as the wildtype in terms of egg counts. The experiments were repeated for three times under three different (unknown) conditions, and perhaps time points. This is certainly a random effect. Furthermore, in the hatching rate model, an egg is the basic unit but eggs from the same mosquito are corrected. Therefore I have to introduce another layer of random effect on the mosquito level. \n",
    "\n",
    "A simple linear random effect model looks like this:\n",
    "$$y_{ij}=\\mu+\\tau_j+\\epsilon_{ij}$$\n",
    "$$i=1,2,...,n, j=1,2,..., k$$\n",
    "In this model there are $k$ random groups each with $n$ observations. $\\mu$ is the overall mean, and $\\epsilon_{ij}\\sim N(0, \\sigma^2)$ is the i.i.d. individual error term. In addition, $\\tau_j\\sim N(0, \\sigma_{random}^2)$ is another random error term for all members from group $j$. By definition the likelihood it is the joint pdf of all $k*n$ observations: \n",
    "$$L(\\underline{\\theta})=f(y_{11}, ..., y_{n1}, y_{12}, ..., y_{n2}, ..., y_{1k}, ..., y{nk})$$\n",
    "It is important to understand that individuals from the same random group are not independent as they share the same value of $\\tau_j$. Therefore\n",
    "$$L(\\underline{\\theta})=f_{group_1}(y_{11}, ..., y_{n1})f_{group_2}(y_{11}, ..., y_{n1})...f_{group_k}(y_{11}, ..., y_{n1})$$\n",
    "where $f_{group_j}$ is the joint pdf of the observations from the random group $j$. We cannot simplify the likelihood further. \n",
    "\n",
    "If we zoom into the joint pdf of group $j$, \n",
    "$$f_{group_j}(y_{1j}, ..., y_{nj})=\\int f_{group_j}(y_{1j}, ..., y_{nj})f(\\tau_j)d\\tau_j$$\n",
    "as $\\tau_j$ is a nuisance variable that needs to be integrated out. In fact, $Y_{ij}|\\tau_j\\sim N(\\mu+\\tau_j, \\sigma^2)$. As a result, the likelihood is a product of integrals and often can only be evaluated numerically. There are algorithms to approximate integrals (e.g. Laplace approximation), some of which are implemented in <code>lmer()</code> or <code>glmer()</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe4f67c-89ee-4fdf-80e6-51bd63d24dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R-4.3.1",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
